# Business RAG Chatbot

A production-ready Retrieval-Augmented Generation (RAG) chatbot system designed for SaaS knowledge bases. This system enables users to query company documents, FAQs, and support articles with contextual, AI-powered responses.

## ğŸ¯ Features

- **Document Ingestion**: Load and process PDF, TXT, and Markdown files
- **Vector Search**: Semantic search using FAISS or Chroma vector stores
- **Conversational AI**: Multi-turn conversations with context awareness
- **Query Refinement**: Automatic query optimization for better retrieval
- **Source Citation**: Transparent source attribution for all answers
- **Web UI**: Streamlit-based interactive chat interface
- **REST API**: FastAPI backend for integration
- **Docker Support**: Containerized deployment ready
- **Flexible Embeddings**: Support for OpenAI or Hugging Face embeddings

## ğŸ“‹ Requirements

- Python 3.10+
- OpenAI API key (for LLM and optional embeddings)
- 4GB+ RAM (for local embeddings)

## ğŸš€ Quick Start

### 1. Installation

```bash
# Clone the repository
git clone https://github.com/yourusername/business-rag-chatbot.git
cd business-rag-chatbot

# Install dependencies
pip install -r requirements.txt
```

### 2. Configuration

Copy the example environment file and configure:

```bash
cp .env.example .env
```

Edit `.env` and set your OpenAI API key:

```env
OPENAI_API_KEY=your_api_key_here
LLM_MODEL=gpt-3.5-turbo
EMBEDDING_MODEL=openai
VECTOR_STORE_TYPE=faiss
```

### 3. Ingest Documents

Place your documents in the `data/` directory, then run:

```bash
python ingest.py --data-dir ./data
```

This will:
- Load all PDF, TXT, and MD files from the directory
- Split them into chunks
- Generate embeddings
- Create and save the vector store

### 4. Run the Chatbot

**Option A: Streamlit UI**

```bash
streamlit run app.py
```

Open your browser to `http://localhost:8501`

**Option B: FastAPI Server**

```bash
python api.py
```

API will be available at `http://localhost:8000`

## âš™ï¸ Configuration

### Environment Variables

| Variable | Description | Default |
|----------|-------------|---------|
| `OPENAI_API_KEY` | OpenAI API key (required) | - |
| `LLM_MODEL` | LLM model name | `gpt-3.5-turbo` |
| `LLM_TEMPERATURE` | LLM temperature | `0.7` |
| `LLM_MAX_TOKENS` | Max tokens per response | `1000` |
| `EMBEDDING_MODEL` | Embedding type (`openai` or `sentence-transformers`) | `openai` |
| `EMBEDDING_MODEL_NAME` | Hugging Face model name | `all-MiniLM-L6-v2` |
| `VECTOR_STORE_TYPE` | Vector store (`faiss` or `chroma`) | `faiss` |
| `VECTOR_STORE_PATH` | Path to vector store | `./vectorstore` |
| `CHUNK_SIZE` | Text chunk size | `1000` |
| `CHUNK_OVERLAP` | Chunk overlap | `200` |
| `TOP_K_RETRIEVAL` | Number of docs to retrieve | `5` |
| `RERANK_RESULTS` | Enable reranking | `false` |

### Configuration File

You can also use a Python config file (`src/config.py`) for programmatic configuration.

## ğŸ³ Docker Deployment

### Build and Run

```bash
# Build the image
docker build -t business-rag-chatbot .

# Run with docker-compose
docker-compose up -d
```

### Docker Compose Services

- **rag-api**: FastAPI server on port 8000
- **rag-ui**: Streamlit UI on port 8501

Both services share the same data and vectorstore volumes.

### Environment Variables in Docker

Create a `.env` file or set environment variables:

```bash
docker run -e OPENAI_API_KEY=your_key business-rag-chatbot
```

## ğŸ§ª Testing

Run the test suite:

```bash
pytest tests/ -v
```

Test coverage includes:
- Document ingestion and chunking
- Vector store operations
- Embedding generation
- RAG chain functionality (requires API key)

## ğŸ“ Project Structure

```
business-rag-chatbot/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ config.py              # Configuration management
â”‚   â”œâ”€â”€ ingestion.py            # Document loading and chunking
â”‚   â”œâ”€â”€ embeddings.py           # Embedding model management
â”‚   â”œâ”€â”€ vectorstore.py          # Vector store operations
â”‚   â”œâ”€â”€ rag_chain.py            # RAG chain implementation
â”‚   â””â”€â”€ query_refinement.py     # Query optimization
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_ingestion.py
â”‚   â”œâ”€â”€ test_vectorstore.py
â”‚   â””â”€â”€ test_rag_chain.py
â”œâ”€â”€ data/                       # Document storage
â”‚   â”œâ”€â”€ sample.txt
â”‚   â””â”€â”€ README.md
â”œâ”€â”€ ingest.py                   # Ingestion CLI script
â”œâ”€â”€ app.py                      # Streamlit UI
â”œâ”€â”€ api.py                      # FastAPI server
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ docker-compose.yml
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ .env.example
â””â”€â”€ README.md
```

## ğŸ“§ Support

- Telegram: https://t.me/az_tekDev
- Twitter: https://x.com/az_tekDev

